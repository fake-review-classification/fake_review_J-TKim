{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baS0lOejtaxT"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from platform import python_version\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transformers\n",
    "\n",
    "from preprocessing import preprocessing\n",
    "from tokenize_and_pad_text import *\n",
    "from train_model import KimCNN, train_test_model\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('version')\n",
    "print(f\"python version=={python_version()}\")\n",
    "print(f\"pandas=={pd.__version__}\")\n",
    "print(f\"numpy=={np.__version__}\")\n",
    "print(f\"torch=={torch.__version__}\")\n",
    "print(f\"sklearn=={sklearn.__version__}\")\n",
    "print(f\"transformers=={transformers.__version__}\")\n",
    "print(f\"matplotlib=={matplotlib.__version__}\",end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = transformers.BertModel\n",
    "tokenizer_class = transformers.BertTokenizer\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "target_columns = ['label']\n",
    "\n",
    "max_seq = 128\n",
    "bert_batch_size = 16\n",
    "\n",
    "kernel_num = 3\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dropout = 0.5\n",
    "static = True\n",
    "\n",
    "n_epochs = 50\n",
    "patience = 5\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "k_fold = 5\n",
    "optimizer = torch.optim.Adam\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "data_path = '../reviews.csv'\n",
    "data_name = data_path.split('/')[-1]\n",
    "print(f'use {data_name} data', end='\\n')\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ratio_threshold, dist_type=None, dist_threshold=1, random_seed=42):\n",
    "    \n",
    "    assert 0 <= ratio_threshold <= 1, 'ratio threshold must between 0 and 1'\n",
    "    \n",
    "    print(f'start threshold {ratio_threshold}!!!')\n",
    "    \n",
    "    preprocessing_class = preprocessing(df, ratio_threshold=ratio_threshold, dist_type=dist_type, dist_threshold=dist_threshold, random_seed=random_seed)\n",
    "\n",
    "    df_train, df_val, df_test = preprocessing_class.preprocessing_all()\n",
    "    \n",
    "#     import json\n",
    "#     all_list = list(set(real_dict.keys()) & set(fake_dict.keys()))\n",
    "#     with open('all_word.json', 'w') as outfile:\n",
    "#         json.dump({'word':all_list}, outfile)\n",
    "    \n",
    "    print('make train data ...')\n",
    "    x_train, y_train = tokenize_and_pad_text_bert(df_train, device, model_class, tokenizer_class, pretrained_weights,\n",
    "                                                max_seq=max_seq, batch_size=bert_batch_size, target_columns=target_columns)\n",
    "\n",
    "    print('make valid data ...')\n",
    "    x_val, y_val = tokenize_and_pad_text_bert(df_val, device, model_class, tokenizer_class, pretrained_weights,\n",
    "                                                max_seq=max_seq, batch_size=bert_batch_size, target_columns=target_columns)\n",
    "\n",
    "    print('make test data ...')\n",
    "    x_test, y_test = tokenize_and_pad_text_bert(df_test, device, model_class, tokenizer_class, pretrained_weights,\n",
    "                                                max_seq=max_seq, batch_size=bert_batch_size, target_columns=target_columns)\n",
    "\n",
    "    embed_num = x_train.shape[1]\n",
    "    embed_dim = x_train.shape[2]\n",
    "    class_num = y_train.shape[1]\n",
    "\n",
    "    auc_score_list = []\n",
    "    \n",
    "    for fold in range(k_fold):\n",
    "        model = KimCNN(\n",
    "            embed_num=embed_num,\n",
    "            embed_dim=embed_dim,\n",
    "            class_num=class_num,\n",
    "            kernel_num=kernel_num,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            dropout=dropout,\n",
    "            static=static,\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        # train and test\n",
    "        review_classification_model = train_test_model(model, n_epochs, batch_size, lr, optimizer, loss_fn)\n",
    "        # auc_score_list = review_classification_model.kfold_train_test(x_train, y_train, x_test, y_test, kwargs, k_fold=5)\n",
    "\n",
    "        review_classification_model.train(x_train, y_train, x_val, y_val, fold=0, patience=patience)\n",
    "        y_test_np, y_preds_np = review_classification_model.test(x_test, y_test)\n",
    "\n",
    "        auc_score = roc_auc_score(y_test_np, y_preds_np, average=None)\n",
    "        print(f'k_fold: {fold+1}/{k_fold},\\tauc score: {auc_score}')\n",
    "        auc_score_list.append(auc_score)\n",
    "        \n",
    "    print(f'ratio_threshold: {ratio_threshold},\\tdist_type and dist_threshold: {dist_type} {dist_threshold}\\nauc score: {np.mean(auc_score_list)}, std: {np.std(auc_score_list)}')\n",
    "    return auc_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_threshold = 0.9\n",
    "dist_type = 'cosine_similarity' # input cosine_similarity, eucliean_distance or None if you don't want to delite word considering the distance\n",
    "dist_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(ratio_threshold, dist_type, dist_threshold)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "identifying-hate-speech-with-bert-and-cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
