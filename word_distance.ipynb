{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing import preprocessing\n",
    "from tokenize_and_pad_text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "data_path = '../reviews.csv'\n",
    "data_name = data_path.split('/')[-1]\n",
    "print(f'use {data_name} data', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = transformers.BertModel\n",
    "tokenizer_class = transformers.BertTokenizer\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "\n",
    "max_seq = 128\n",
    "bert_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_word.json', 'r') as f:\n",
    "    word_dict = json.load(f)\n",
    "    all_word_list = word_dict['word']\n",
    "    \n",
    "pad_word_list = []\n",
    "for idx in range(0, len(all_word_list), max_seq):\n",
    "    pad_word_list.append(all_word_list[idx:idx+max_seq])\n",
    "    \n",
    "df = pd.DataFrame({'review': pad_word_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(df, max_seq, tokenizer):\n",
    "    return [\n",
    "        # tokenizer.encode(text, add_special_tokens=True)[:max_seq] for text in df.comment_text.values\n",
    "        tokenizer.encode(text, add_special_tokens=True)[:max_seq] for text in df.review.values\n",
    "    ]\n",
    "\n",
    "\n",
    "def pad_text(tokenized_text, max_seq):\n",
    "    return np.array([el + [0] * (max_seq - len(el)) for el in tokenized_text])\n",
    "\n",
    "\n",
    "def tokenize_and_pad_text(df, max_seq, tokenizer):\n",
    "    tokenized_text = tokenize_text(df, max_seq, tokenizer)\n",
    "    padded_text = pad_text(tokenized_text, max_seq)\n",
    "    return torch.tensor(padded_text)\n",
    "\n",
    "\n",
    "def targets_to_tensor(df, target_columns):\n",
    "    return torch.tensor(df[target_columns].values, dtype=torch.float32)\n",
    "\n",
    "def tokenize_and_pad_text_bert(df, device, model_class, tokenizer_class, pretrained_weights, max_seq=128, batch_size=16, target_columns=['label']):\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    bert_model = model_class.from_pretrained(pretrained_weights).to(device)\n",
    "    bert_model.eval()\n",
    "\n",
    "    data_indices = tokenize_and_pad_text(df, max_seq, tokenizer)\n",
    "    data_indices = data_indices.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_data = bert_model(data_indices[:batch_size])[0]\n",
    "        for idx in tqdm(range(batch_size, len(data_indices), batch_size)):\n",
    "            x_data = torch.cat((x_data, bert_model(data_indices[idx:idx+batch_size])[0]), 0)\n",
    "\n",
    "    return x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenize_and_pad_text_bert(df, device, model_class, tokenizer_class, pretrained_weights,\n",
    "                                                max_seq=max_seq, batch_size=bert_batch_size, target_columns=None)\n",
    "x_train = x_train.reshape(x_train.shape[0]*x_train.shape[1], x_train.shape[2])\n",
    "x_train.shape\n",
    "x_train = x_train.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_x_train = []\n",
    "for emb in x_train:\n",
    "    reshape_x_train.append(emb.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "file_list = os.listdir('./json_folder')\n",
    "\n",
    "for idx1, word1 in tqdm(enumerate(all_word_list)):\n",
    "    file_name = f'{word1}.json'\n",
    "    if file_name in file_list:\n",
    "        continue\n",
    "    \n",
    "    dic = {}\n",
    "    \n",
    "    for idx2, word2 in enumerate(all_word_list):\n",
    "        cs = cosine_similarity(x_train[idx1].view([1, 768]), x_train[idx2].view([1, 768]))[0][0]\n",
    "        ed = euclidean_distances(x_train[idx1].view([1, 768]), x_train[idx2].view([1, 768]))[0][0]\n",
    "        dic[word2] = [float(cs), float(ed)]\n",
    "    \n",
    "    with open(f'./json_folder/{file_name}', 'w', encoding='utf-8') as make_file:\n",
    "        json.dump({word1: dic}, make_file, indent='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
